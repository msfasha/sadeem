# Example SparkApplication for writing Iceberg tables
# Usage: kubectl apply -f spark-iceberg-example.yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: spark-iceberg-write-example
  namespace: default
spec:
  type: Python
  mode: cluster
  image: "apache/spark-py:3.5.0"
  imagePullPolicy: Always
  mainApplicationFile: "local:///opt/spark/examples/src/main/python/sql/basic.py"
  sparkVersion: "3.5.0"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 3.5.0
    serviceAccount: dataplane-spark-operator
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 3.5.0
  sparkConf:
    "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
    "spark.sql.catalog.demo": "org.apache.iceberg.spark.SparkCatalog"
    "spark.sql.catalog.demo.type": "rest"
    "spark.sql.catalog.demo.uri": "http://dataplane-iceberg-catalog:8181"
    "spark.sql.catalog.demo.io-impl": "org.apache.iceberg.aws.s3.S3FileIO"
    "spark.sql.catalog.demo.s3.endpoint": "http://dataplane-minio:9000"
    "spark.sql.catalog.demo.s3.access-key-id": "minioadmin"
    "spark.sql.catalog.demo.s3.secret-access-key": "minioadmin"
    "spark.sql.catalog.demo.s3.path-style-access": "true"




