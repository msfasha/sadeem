Below is a **concrete minimal BOM** and a **production-capable configuration pattern** for an on-prem open-source analytics + ML platform built around **Spark + Iceberg + MinIO + Trino + Airflow + MLflow/JupyterHub + Keycloak**. I’m optimizing for “small, reasonable, covers ~80%,” and avoiding fragile single points of failure where it matters.

---

## 1) Minimal BOM options

### Option A — Smallest “real production” footprint (8 physical servers)

This is the best balance of **cost, simplicity, and resilience**.

**Core assumptions**

* Tens of TB initially (scales to low hundreds TB by adding storage/worker nodes)
* Dozens of analysts/engineers, light-to-moderate concurrency
* Batch ETL + interactive BI SQL + ML experimentation (not heavy real-time serving)

#### A1. Kubernetes control plane (3 nodes)

These should be stable and not overloaded.

* **3 x Control-plane nodes**

  * CPU: **16 cores**
  * RAM: **64 GB**
  * Disk: **2 x 1.92 TB NVMe (RAID1)** for OS + etcd durability
  * Network: **2 x 10GbE** recommended (or 1 x 10GbE minimum)

Purpose: k8s control plane + core platform services that you don’t want to preempt.

#### A2. Compute/Services worker nodes (3 nodes)

Run Spark executors/drivers, Trino workers/coordinator, Airflow, notebooks, MLflow.

* **3 x Worker nodes**

  * CPU: **32 cores**
  * RAM: **256 GB**
  * Disk: **2 x 1.92 TB NVMe** (local shuffle/temp + caching)
  * Network: **2 x 10GbE**

Purpose: the “engine room” for Spark/Trino and platform services.

#### A3. Object storage (MinIO) nodes (2 nodes)

For a minimal production cluster, **2 nodes is workable** but less ideal than 4. If you can afford it, go 4 (see Option B).

* **2 x MinIO nodes**

  * CPU: **16 cores**
  * RAM: **128 GB** (MinIO benefits from RAM for caching/metadata)
  * Disk: **8 x 16 TB HDD** (or 12 TB if budget constrained)
  * OS/Cache: **1 x 1.92 TB NVMe** (OS + MinIO cache/tier)
  * Network: **2 x 25GbE** (10GbE minimum)

Raw: 2 nodes × 8 × 16 TB = **256 TB raw**
Usable depends on erasure coding / parity; plan roughly **~50–65% usable** as a conservative planning number at this small node count.

**Summary**: 3 control + 3 compute + 2 storage = **8 servers**.

---

### Option B — Still small, but materially better storage resilience (10 physical servers)

If you can do **4 MinIO nodes**, do it. MinIO erasure coding and operational tolerance are better.

* Control plane: **3 nodes** (same as above)
* Compute: **3 nodes** (same as above)
* MinIO: **4 nodes**

  * CPU 16 cores / RAM 128 GB
  * **8 x 16 TB HDD + 1 x NVMe**
  * Network: **25GbE preferred**

Raw: 4 × 8 × 16 TB = **512 TB raw**, and you’ll get a much healthier usable/resilience profile.

**Summary**: 10 servers.

---

### Option C — Ultra-minimal (not recommended for “production,” but can run)

* 3 nodes total, everything combined: you will have noisy-neighbor issues and difficult maintenance windows.
* I’m not detailing it because you asked “production-capable.”

---

## 2) Network, rack, and baseline standards

* **10GbE is the minimum** for this stack. If you can do **25GbE for MinIO**, you will feel it immediately.
* Separate VLANs if possible:

  * Storage traffic (S3/MinIO)
  * Cluster/service traffic (k8s/Trino/Spark)
* Time sync: NTP everywhere.
* Use SSD/NVMe for:

  * OS disks (avoid slow boot/control plane issues)
  * Spark shuffle / temp
  * Trino spill (if needed)
* Backups:

  * MinIO bucket replication or scheduled backups to a second storage target (even a smaller one).

---

## 3) Logical architecture and “what runs where”

### Kubernetes

Run **everything on Kubernetes** for consistent deployment/upgrade patterns:

* Spark (Spark Operator)
* Trino
* Airflow
* MLflow
* JupyterHub
* Keycloak
* Monitoring (Prometheus/Grafana)
* Logging (optional: OpenSearch/ELK)

MinIO can run **on k8s** or **bare metal**. For small teams, I often prefer **bare metal MinIO** (simpler performance and disk management), but k8s is fine if you already have strong k8s storage discipline.

---

## 4) Concrete component-by-component configuration pattern

### 4.1 MinIO (S3-compatible object storage)

**Buckets (minimum)**

* `bronze/` raw ingest
* `silver/` cleaned/conformed
* `gold/` curated marts
* `warehouse/` (if you separate)
* `mlflow/` artifacts
* `logs/` (optional)

**Hardening**

* Enable TLS for S3 endpoints
* Use bucket policies (service accounts per component)
* Versioning enabled for critical buckets (Iceberg metadata, MLflow artifacts)

**Performance**

* NVMe as cache/tier for hot data if supported in your design
* 25GbE or dual 10GbE recommended

---

### 4.2 Iceberg (table format) + Catalog

You need a catalog. Two common “small but robust” choices:

**Choice 1: Iceberg REST Catalog** (recommended for simplicity and modernity)

* Run a REST catalog service in k8s.
* Store metadata in a small RDBMS (Postgres).

**Choice 2: Hive Metastore**

* Familiar, widely supported.
* Slightly more legacy operationally, but stable.

**Data layout**

* Use Iceberg tables in MinIO:

  * `s3a://<bucket>/iceberg/db/table/...`
* Partition by common filters (date, org unit, etc.), but avoid over-partitioning.

---

### 4.3 Spark (batch + ML workhorse)

**Deployment**

* Kubernetes Spark Operator
* Use dynamic allocation where it makes sense

**Baseline Spark settings (practical defaults)**

* `spark.sql.adaptive.enabled=true` (AQE)
* `spark.sql.shuffle.partitions` tuned per workload (start 200–800 for medium clusters)
* `spark.dynamicAllocation.enabled=true` with min/max executors by job class
* `spark.local.dir` mapped to NVMe for shuffle
* Use `s3a://` connector to MinIO:

  * `fs.s3a.endpoint=https://minio:9000`
  * `fs.s3a.path.style.access=true`
  * TLS + access keys via Kubernetes secrets

**Workload pattern**

* “ELT” jobs land/transform to Iceberg Silver/Gold.
* ML jobs read Iceberg features, write back predictions/features.

---

### 4.4 Trino (interactive SQL for BI)

**Why Trino**: it gives you the concurrency/latency profile Spark SQL typically struggles to deliver for BI.

**Topology**

* 1 coordinator pod (anti-affinity, pinned to stable worker)
* N worker pods (scale horizontally)

**Connectors**

* Iceberg connector pointed at your catalog + MinIO
* Optional: JDBC connectors to legacy systems

**Key settings**

* Enable spill to local NVMe if you have memory pressure
* Size worker memory conservatively; don’t starve the JVM
* Configure resource groups (prevent one BI user from melting the cluster)

---

### 4.5 Airflow (orchestration)

**Deployment**

* Airflow on k8s using Celery or KubernetesExecutor
* Metadata DB: Postgres (HA optional; at minimum, backed up)

**Patterns**

* Use Airflow operators to:

  * Submit SparkApplication CRDs (Spark Operator)
  * Run Trino SQL transformations
  * Trigger data quality checks

**Minimum standards**

* Environment separation: dev/test/prod DAGs or namespaces
* Secrets management via k8s secrets (or Vault later)

---

### 4.6 MLflow (experiment tracking + registry)

**Backend store**

* Postgres (same cluster as others, but separate DB/schema)

**Artifact store**

* MinIO bucket: `s3://mlflow/`

**Access**

* Integrate with Keycloak via OIDC proxy if needed

**Model registry**

* Use MLflow registry for “stage gating” (Staging → Production)
* For serving, start with batch scoring; add a serving layer later if needed

---

### 4.7 JupyterHub (data science workspace)

**Deployment**

* JupyterHub on k8s
* User pods scheduled onto compute nodes
* Persistent volumes: either per-user (NFS/Ceph) or S3-backed workflows (less ideal for notebooks)

**Security**

* SSO via Keycloak OIDC
* Limit resource per user (CPU/RAM) via k8s quotas

---

### 4.8 Keycloak (identity)

**Role**

* Central SSO for:

  * JupyterHub
  * Airflow UI
  * Trino UI (and potentially client auth)
  * MLflow UI

**Baseline**

* OIDC clients per service
* Groups mapped to roles (e.g., `data-engineering`, `data-science`, `bi-analysts`)
* Enforce least privilege at:

  * Trino (resource groups + connector permissions)
  * MinIO (bucket policies per group/service)

---

### 4.9 Monitoring and logging

**Minimum**

* Prometheus + Grafana dashboards:

  * Node exporter, kube-state-metrics
  * Spark metrics (Prometheus sink)
  * Trino JMX exporter
  * MinIO metrics endpoint

**Logging**

* Start with a simple centralized logging (Loki is lightweight) before going full ELK/OpenSearch.

---

## 5) Capacity planning (what this buys you)

With Option A compute (3 workers × 32 cores, 256 GB):

* **Total compute**: ~96 cores, ~768 GB RAM (before overhead)
* Comfortable for:

  * Several concurrent Spark jobs (small/medium)
  * Trino serving BI queries with moderate concurrency (tens of users)
  * Notebooks and ML experiments

For storage, the MinIO tier is usually the first thing you expand. Compute scales linearly by adding worker nodes.

---

## 6) Operational guardrails that prevent pain

1. **Namespaces + quotas** in Kubernetes:

   * `platform/`, `batch/`, `interactive/`, `notebooks/`
2. **Separate Trino from Spark** at the scheduling level:

   * Different node pools or taints/tolerations if possible
3. **Data contracts + quality checks**:

   * Add Great Expectations or Soda Core later; start with lightweight checks in Airflow
4. **Backups**:

   * Postgres backups (Airflow/MLflow/Keycloak/catalog)
   * MinIO bucket replication or periodic snapshots

---

## 7) If you do not want Kubernetes (VM-based pattern)

Still feasible:

* Spark standalone or YARN (I generally prefer avoiding YARN for new builds)
* Trino on dedicated VMs
* Airflow + Postgres on VMs
* MinIO on bare metal
  This can be simpler initially, but upgrades and lifecycle management become harder over time.

---

## 8) Summary BOM (quick list)

### Option A (8 servers)

* 3 × (16c / 64GB / 2×1.92TB NVMe RAID1 / 10GbE) – k8s control
* 3 × (32c / 256GB / 2×1.92TB NVMe / 2×10GbE) – compute/services
* 2 × (16c / 128GB / 8×16TB HDD + 1×1.92TB NVMe / 2×25GbE) – MinIO

### Option B (10 servers, preferred)

Same as A but **4 MinIO nodes**.

---

If you tell me **(a)** expected initial data volume and growth (TB/month), **(b)** how many BI users concurrently, and **(c)** whether you need streaming in the first 6 months, I will tune the BOM (cores/RAM/disks) and the default Spark/Trino sizing so it is neither underpowered nor overbuilt.
